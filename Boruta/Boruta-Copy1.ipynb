{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to demonstrate how to use the Boruta algorithm for feature selection.  To start I am going to show how to apply it to a dataset.  I am going to make a dataset with 1000 samples and 4 features.  Of those 4 features, only 2 of them will be useful.  Lets see if Boruta is able to handle this.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-2.591025</td>\n",
       "      <td>-1.223175</td>\n",
       "      <td>0.134147</td>\n",
       "      <td>-1.788159</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.122498</td>\n",
       "      <td>1.180470</td>\n",
       "      <td>-1.902731</td>\n",
       "      <td>0.065675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.917016</td>\n",
       "      <td>0.935056</td>\n",
       "      <td>-0.488524</td>\n",
       "      <td>-0.598798</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.220961</td>\n",
       "      <td>0.015644</td>\n",
       "      <td>0.929275</td>\n",
       "      <td>0.548338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.457067</td>\n",
       "      <td>-1.481252</td>\n",
       "      <td>-0.427098</td>\n",
       "      <td>2.119179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3  target\n",
       "0 -2.591025 -1.223175  0.134147 -1.788159       0\n",
       "1  1.122498  1.180470 -1.902731  0.065675       0\n",
       "2  0.917016  0.935056 -0.488524 -0.598798       1\n",
       "3  2.220961  0.015644  0.929275  0.548338       1\n",
       "4  0.457067 -1.481252 -0.427098  2.119179       1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# make dataset \n",
    "X, y = make_classification(n_samples = 1000, \n",
    "                           n_features=4, \n",
    "                           n_informative=2, \n",
    "                           n_redundant=0, \n",
    "                           random_state=11)\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to fit a RandomForest Classifier to this dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next import Boruta and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 / 100\n",
      "Iteration: 2 / 100\n",
      "Iteration: 3 / 100\n",
      "Iteration: 4 / 100\n",
      "Iteration: 5 / 100\n",
      "Iteration: 6 / 100\n",
      "Iteration: 7 / 100\n",
      "Iteration: 8 / 100\n",
      "Iteration: 9 / 100\n",
      "Iteration: 10 / 100\n",
      "Iteration: 11 / 100\n",
      "Iteration: 12 / 100\n",
      "Iteration: 13 / 100\n",
      "Iteration: 14 / 100\n",
      "Iteration: 15 / 100\n",
      "Iteration: 16 / 100\n",
      "Iteration: 17 / 100\n",
      "Iteration: 18 / 100\n",
      "Iteration: 19 / 100\n",
      "Iteration: 20 / 100\n",
      "Iteration: 21 / 100\n",
      "Iteration: 22 / 100\n",
      "Iteration: 23 / 100\n",
      "Iteration: 24 / 100\n",
      "Iteration: 25 / 100\n",
      "Iteration: 26 / 100\n",
      "Iteration: 27 / 100\n",
      "Iteration: 28 / 100\n",
      "Iteration: 29 / 100\n",
      "Iteration: 30 / 100\n",
      "Iteration: 31 / 100\n",
      "Iteration: 32 / 100\n",
      "Iteration: 33 / 100\n",
      "Iteration: 34 / 100\n",
      "Iteration: 35 / 100\n",
      "Iteration: 36 / 100\n",
      "Iteration: 37 / 100\n",
      "Iteration: 38 / 100\n",
      "Iteration: 39 / 100\n",
      "Iteration: 40 / 100\n",
      "Iteration: 41 / 100\n",
      "Iteration: 42 / 100\n",
      "Iteration: 43 / 100\n",
      "Iteration: 44 / 100\n",
      "Iteration: 45 / 100\n",
      "Iteration: 46 / 100\n",
      "Iteration: 47 / 100\n",
      "Iteration: 48 / 100\n",
      "Iteration: 49 / 100\n",
      "\n",
      "\n",
      "BorutaPy finished running.\n",
      "\n",
      "Iteration: \t50 / 100\n",
      "Confirmed: \t2\n",
      "Tentative: \t0\n",
      "Rejected: \t2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BorutaPy(alpha=0.05,\n",
       "         estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                          criterion='gini', max_depth=5,\n",
       "                                          max_features='auto',\n",
       "                                          max_leaf_nodes=None,\n",
       "                                          min_impurity_decrease=0.0,\n",
       "                                          min_impurity_split=None,\n",
       "                                          min_samples_leaf=1,\n",
       "                                          min_samples_split=2,\n",
       "                                          min_weight_fraction_leaf=0.0,\n",
       "                                          n_estimators=48, n_jobs=None,\n",
       "                                          oob_score=False,\n",
       "                                          random_state=<mtrand.RandomState object at 0x1a1da4c558>,\n",
       "                                          verbose=0, warm_start=False),\n",
       "         max_iter=100, n_estimators='auto', perc=100,\n",
       "         random_state=<mtrand.RandomState object at 0x1a1da4c558>,\n",
       "         two_step=True, verbose=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from boruta import BorutaPy\n",
    "\n",
    "feat_selector = BorutaPy(clf, n_estimators='auto', verbose=1, random_state=1)\n",
    "feat_selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it confirmed 2 of the features and rejected 2 features.  Now lets see which features it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False, False])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_selector.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_selector.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boruta says that the first and second feature are useful and I should remove the rest of the features.  Now lets use the `transform` method to remove those features that are not useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.5910246 , -1.22317524],\n",
       "       [ 1.12249791,  1.18046999],\n",
       "       [ 0.91701607,  0.93505646],\n",
       "       [ 2.22096146,  0.01564356],\n",
       "       [ 0.45706667, -1.48125216]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_filtered = feat_selector.transform(X)\n",
    "X_filtered[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I tell check if Boruta did a good job or not.  I am going to fit a random forest classifier on each individual feature with a small max depth (to prevent overfitting on the single feature).  I'm also going to make boxplots showing the distribution of the feature for the 2 classes.  I am expecting the first 2 features to have higher accuracy and have more separation between the 2 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "clf = RandomForestClassifier(max_depth=2)\n",
    "for i in range(4):\n",
    "    clf.fit(df[i].values.reshape(-1, 1), df['target'])\n",
    "    sns.boxplot(x = 'target', y = i, data = df)\n",
    "    plt.title(f\"Accuracy: {clf.score(df[i].values.reshape(-1, 1), df['target'])}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first feature has a relatively high accuracy (90%) and the box plots show the 2 classes are very different for this feature.  The second features has the next highest accuracy (67%).  This makes sense why Boruta chose these as important features. \n",
    "\n",
    "## Theory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see the theory of Boruta.  \n",
    "\n",
    "#### Step 1: Make Permutations of each Feature\n",
    "\n",
    "Boruta starts by taking permutations of each feature.  I am going to call each of these new features a shadow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>shadow_0</th>\n",
       "      <th>shadow_1</th>\n",
       "      <th>shadow_2</th>\n",
       "      <th>shadow_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-2.591025</td>\n",
       "      <td>-1.223175</td>\n",
       "      <td>0.134147</td>\n",
       "      <td>-1.788159</td>\n",
       "      <td>-1.828319</td>\n",
       "      <td>-0.636258</td>\n",
       "      <td>1.071065</td>\n",
       "      <td>0.385510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.122498</td>\n",
       "      <td>1.180470</td>\n",
       "      <td>-1.902731</td>\n",
       "      <td>0.065675</td>\n",
       "      <td>0.300768</td>\n",
       "      <td>-2.208547</td>\n",
       "      <td>1.503905</td>\n",
       "      <td>0.355866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.917016</td>\n",
       "      <td>0.935056</td>\n",
       "      <td>-0.488524</td>\n",
       "      <td>-0.598798</td>\n",
       "      <td>-1.640707</td>\n",
       "      <td>0.505145</td>\n",
       "      <td>-0.199718</td>\n",
       "      <td>1.827682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.220961</td>\n",
       "      <td>0.015644</td>\n",
       "      <td>0.929275</td>\n",
       "      <td>0.548338</td>\n",
       "      <td>-1.600522</td>\n",
       "      <td>-0.667427</td>\n",
       "      <td>0.578215</td>\n",
       "      <td>0.629989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.457067</td>\n",
       "      <td>-1.481252</td>\n",
       "      <td>-0.427098</td>\n",
       "      <td>2.119179</td>\n",
       "      <td>-0.154074</td>\n",
       "      <td>0.631955</td>\n",
       "      <td>0.076234</td>\n",
       "      <td>-1.896298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3  shadow_0  shadow_1  shadow_2  \\\n",
       "0 -2.591025 -1.223175  0.134147 -1.788159 -1.828319 -0.636258  1.071065   \n",
       "1  1.122498  1.180470 -1.902731  0.065675  0.300768 -2.208547  1.503905   \n",
       "2  0.917016  0.935056 -0.488524 -0.598798 -1.640707  0.505145 -0.199718   \n",
       "3  2.220961  0.015644  0.929275  0.548338 -1.600522 -0.667427  0.578215   \n",
       "4  0.457067 -1.481252 -0.427098  2.119179 -0.154074  0.631955  0.076234   \n",
       "\n",
       "   shadow_3  \n",
       "0  0.385510  \n",
       "1  0.355866  \n",
       "2  1.827682  \n",
       "3  0.629989  \n",
       "4 -1.896298  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(11)\n",
    "X = df.drop('target', axis = 1)\n",
    "X_shadow = X.apply(np.random.permutation)\n",
    "X_shadow.columns = ['shadow_' + str(feat) for feat in X.columns]\n",
    "\n",
    "X_boruta = pd.concat([X, X_shadow], axis = 1)\n",
    "X_boruta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Fit Random Forest Classifier to new dataset \n",
    "\n",
    "I am now going to fit a random forest classifier to my new dataset and compare the feature importance of my original features to my datasets that have permutations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=11, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth = 5, random_state = 11)\n",
    "clf.fit(X_boruta, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66749262, 0.18664663, 0.03365685, 0.02609232])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature importance of original features \n",
    "feat_imp_X = clf.feature_importances_[:len(X.columns)]\n",
    "feat_imp_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02652004, 0.01652293, 0.02554006, 0.01752856])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # feature importance of random data features \n",
    "feat_imp_shadow = clf.feature_importances_[len(X.columns):]\n",
    "feat_imp_shadow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Compare feature importance of original dataset to random data\n",
    "\n",
    "I now want to see which features have higher feature importance than most important random feature.  The idea is that if a feature is not more important than the random features I should not include it.  In the above example, the most important feature importance is 0.0265 - we see that this is greater than the last original feature because it has a feature importance of 0.0260.  We can verify this with the below code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits = feat_imp_X > feat_imp_shadow.max()\n",
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it says the first 3 features are useful and the last feature is not useful.  However running this a single time, should not be trusted.  We took random permutations of each feature so we should run this multiple times.  When I used the Boruta package, it ran 50 iterations.  I am now going to run it a second time with a different random seed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False,  True])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(12)\n",
    "X = df.drop('target', axis = 1)\n",
    "X_shadow = X.apply(np.random.permutation)\n",
    "X_shadow.columns = ['shadow_' + str(feat) for feat in X.columns]\n",
    "\n",
    "X_boruta = pd.concat([X, X_shadow], axis = 1)\n",
    "clf.fit(X_boruta, y)\n",
    "feat_imp_X = clf.feature_importances_[:len(X.columns)]\n",
    "feat_imp_shadow = clf.feature_importances_[len(X.columns):]\n",
    "hits = feat_imp_X > feat_imp_shadow.max()\n",
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that running it a second time, it gives a different subset of features to keep.  I am now going to put this in a for loop and run it 20 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/jeffreyherman/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "### repeat 20 times\n",
    "hits = np.empty(4)\n",
    "for iter_ in range(20):\n",
    "   ### make X_shadow by randomly permuting each column of X\n",
    "   np.random.seed(iter_)\n",
    "   X_shadow = X.apply(np.random.permutation)\n",
    "   X_boruta = pd.concat([X, X_shadow], axis = 1)\n",
    "   ### fit a random forest (suggested max_depth between 3 and 7)\n",
    "   clf = RandomForestClassifier(max_depth = 5, random_state = 42)\n",
    "   clf.fit(X_boruta, y)\n",
    "   ### store feature importance\n",
    "   feat_imp_X = clf.feature_importances_[:len(X.columns)]\n",
    "   feat_imp_shadow = clf.feature_importances_[len(X.columns):]\n",
    "   ### compute hits for this trial and add to counter\n",
    "   hits += (feat_imp_X > feat_imp_shadow.max()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.  , 1.  , 0.1 , 0.15])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits / 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that of the 20 iterations feature 1 and feature 2 were selected every time.  Feature 3 was selected 10% of the time and feature 4 was selected 15% of the time.  The key component of Boruta is that instead of having the features compete against each other, they are competing against randomized versions of each other.  \n",
    "\n",
    "#### Resources \n",
    "- https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a\n",
    "- https://github.com/scikit-learn-contrib/boruta_py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
